{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.gstatic.com/devrel-devsite/v2355ed73ae6b243a36832e70498f2cd0b3f352ff30481ebdfdc56826b566bf8a/tensorflow/images/lockup.svg)\n",
    "\n",
    "До сегодняшнего дня мы собирали различные довольно простые модели в высокоуровневом API для [tensorflow](https://www.tensorflow.org/) под названием Keras. Сегодня, наконец, пришло время поиграться с голым ~~королём~~ функционалом библиотеки и посмотреть, как в нём собираются и обучаются самые простые модели. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__  # ОУУУУ ЩИИИИТ, ВТОРАЯ ВЕРСИЯ!!! АААААААААААА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Tensoflow teaser\n",
    "\n",
    "Давайте реализуем на `numpy` и на `tensorflow` функцию для поиска суммы квадратов первых $N$ чисел и посмотрим насколько быстро эти функции работают."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_sum_squares(N):\n",
    "    return np.sum(np.arange(N)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466 ms ± 15.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "N = 10**8\n",
    "np_sum_squares(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow из-за того, что мы сначала задаём структуру вычислений, а после осуществляем их. Работает медленнее, чем numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_sum_squares(N):\n",
    "    N = tf.constant(N, dtype='int64')\n",
    "    return tf.reduce_sum((tf.range(N)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "650 ms ± 27.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "N = 10**8\n",
    "tf_sum_squares(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Более того, когда вы уже задали свой граф вычислений и убедились, что всё работает, можно вызвать декоратор `tf.function`. [Он сделает работу заданного графа](https://www.tensorflow.org/tutorials/customization/performance) более оптимальной и ещё ускорит код. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def tf_sum_squares(N):\n",
    "    N = tf.constant(N, dtype='int64')\n",
    "    return tf.reduce_sum((tf.range(N)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 5.39 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "141 µs ± 114 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "N = 10**8\n",
    "tf_sum_squares(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если сравнивать время, то мы видим значительный прогресс в скорости вычислений даже для простой задачи. Данный прирост будет значительно большим для сложных и тяжелых по памяти рассчетов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Тензоры и базовые операции над ними\n",
    "\n",
    "Основной объект в tensorflow - это **тензор.** Или многомерный массив чисел. Чтобы не нужно было учить кучу новых команд, tensorflow косплеит numpy. \n",
    "\n",
    "```\n",
    "    np.zeros -> tf.zeros\n",
    "    np.sin -> tf.sin\n",
    "    np.mean -> tf.reduce_mean\n",
    "    np.arange -> tf.range\n",
    "    np.cumsum -> tf.cumsum\n",
    "```\n",
    "\n",
    "Правда говоря, не совсем косплеит. Но чаще всего оказываетс довольно близок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# можно задать тензор из нулей\n",
    "tf.zeros([3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# или из единиц, но уже более сложной размерности\n",
    "tf.ones([3, 4, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# можно сгенерировать тензор из нормального распределения\n",
    "tf.random.normal([2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# можно просто сделать залифку тензора какими-то числами\n",
    "tf.fill([2, 2], 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# можно создать матрицу в numpy\n",
    "x = np.array([[1, 2, 3, 4],\n",
    "              [4, 3, 2, 1]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# а потом перевести её в тензор\n",
    "tf.convert_to_tensor(x, tf.float32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# или можно просто сразу создать тензор \n",
    "tf.constant([[1,  2,  3,  4],\n",
    "             [5,  6,  7,  8],\n",
    "             [9, 10, 11, 12]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# наверное, стоит уточнить тип данных в тензоре\n",
    "x = tf.constant([[1,  2,  3,  4],\n",
    "                 [5,  6,  7,  8],\n",
    "                 [9, 10, 11, 12]], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все срезы, операции, размерности работают как в numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[x > 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.constant([[12, 11, 10, 9],\n",
    "                  [8, 7, 6, 5],\n",
    "                  [4, 3, 2, 1]], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x**y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.math.exp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Когда надоело работать с тензором, можно вернуться назад в numpy формат\n",
    "x.numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# тип у тензоров может быть разный, но всегда один\n",
    "tf.constant(\"hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1:\n",
    "\n",
    "Попробуйте реализовать на tensorflow сигмоиду. \n",
    "\n",
    "$$ \\sigma(x) = \\frac{1}{1 + e^{-x}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваше решение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2:\n",
    "\n",
    "Попробуйте реализовать на tensorflow среднюю квадратичную ошибку. \n",
    "\n",
    "$$ \n",
    "MSE(\\hat y, y) = \\sum_{i=1}^n (\\hat y - y)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваше решение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Производные да градиенты\n",
    "\n",
    "Тензорфлоу может вычислять производные и градиенты автоматически. Для этого используется [`GradientTape.`](https://www.tensorflow.org/tutorials/customization/autodiff)\n",
    "\n",
    "Он проходит по всем операциям, которые фигурируют в графе, и применяет к ним chain rule:\n",
    "\n",
    "$$ {\\partial f(g(x)) \\over \\partial x} = {\\partial f(g(x)) \\over \\partial g(x)}\\cdot {\\partial g(x) \\over \\partial x} $$\n",
    "\n",
    "Мы можем посчитать производную по любому тензору, участвующему в вычислениях."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant([0.3, 1])\n",
    "\n",
    "# опция persistent=True позволяет искать производную много раз, а не один\n",
    "with tf.GradientTape(persistent=True) as t:\n",
    "    t.watch(x)\n",
    "    y = tf.reduce_sum(x)\n",
    "    z = y**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.gradient(z, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.gradient(z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нарисуем функцию и её производную на картинке :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x = tf.linspace(-3., 3., 100)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as t:\n",
    "    t.watch(x)\n",
    "    y = x**2\n",
    "\n",
    "dy_dx = t.gradient(y, x)\n",
    "\n",
    "plt.plot(x, y, label=\"$x^2$\")\n",
    "plt.plot(x, dy_dx, label=r\"$\\frac{dx^2}{dx}$\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3:\n",
    "\n",
    "Реализуйте расчёт градиента для функции \n",
    "\n",
    "$$\n",
    "f(w) = \\prod_{i,j} \\ln(\\ln(w_{ij} + 7) \n",
    "$$\n",
    "\n",
    "в точке `w = [[5,10], [1,2]]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.constant([[5,10], [1,2]], dtype=tf.float32)\n",
    "\n",
    "# Ваше решение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Переменные\n",
    "\n",
    "Следущий важный объект в tensorflow это **переменная.** Она представляет из себя некоторый буфер в памяти, который содержит текущие тензоры. \n",
    "\n",
    "* Переменную можно вставить в любое место графа \n",
    "* Переменные можно использовать, чтобы описать какую-то трансформацию\n",
    "* В процессе расчётов их можно изменять\n",
    "* Обычно их исползуют для описания параметров модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.Variable(initial_value=0.5)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,2,3], dtype=float)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x*w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Более того, переменные хороши тем, что Tensorflow сразу же следит за ними при вычислении производных. Ему не надо указывать с помощью команды `watch`, за каким тензором нужно приглядывать. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4:\n",
    "\n",
    "Реализуйте расчёт градиента для функции \n",
    "\n",
    "$$\n",
    "f(w) = \\prod_{i,j} \\ln(\\ln(w_{ij} + 7) \n",
    "$$\n",
    "\n",
    "в точке `w = [[5,10], [1,2]]`. Отличие от предыдущей задачки такое: надо задать $w$ не как тензор, а как переменную. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.Variable(initial_value=[[5.,10], [1,2]])\n",
    "\n",
    "# Ваше решение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Оптимизация\n",
    "\n",
    "Итак, tf умеет искать производные, осталось научиться применять его для оптимизации. \n",
    "\n",
    "Попробуем решить глупую задачку по конвертации градусов по цельсию в градусы по фаренгейту. По данным будем пытаться восстановить формулу: \n",
    "\n",
    "$$ f = c \\times 1.8 + 32 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "celsius    = np.array([-40, -10,  0,  8, 15, 22,  38],  dtype='float32')\n",
    "fahrenheit = np.array([-40,  14, 32, 46, 59, 72, 100],  dtype='float32')\n",
    "\n",
    "for i,c in enumerate(celsius):\n",
    "    print(\"{} degrees Celsius = {} degrees Fahrenheit\".format(c, fahrenheit[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# параметры модели \n",
    "a = tf.Variable(tf.random.normal([1]), name='bias')\n",
    "b = tf.Variable(tf.random.normal([1]), name='k')\n",
    "\n",
    "a.numpy(), b.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Наша модель\n",
    "def linear_regression(x):\n",
    "    return a + b*x\n",
    "\n",
    "# Ошибка для модели\n",
    "def mean_square(y_pred, y_true):\n",
    "    return tf.reduce_mean((y_pred-y_true)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression(celsius).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_square(linear_regression(celsius), fahrenheit).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# оптимизатор \n",
    "optimizer = tf.optimizers.SGD(learning_rate=0.001)\n",
    "\n",
    "# процесс оптимизации\n",
    "def model_train(X, Y):\n",
    "\n",
    "    # находим loss и пробрасываем градиент\n",
    "    with tf.GradientTape() as g:\n",
    "        pred = linear_regression(X)\n",
    "        loss = mean_square(pred, Y)\n",
    "\n",
    "    # Вычисляем градиенты\n",
    "    gradients = g.gradient(loss, [a, b])\n",
    "    \n",
    "    # Обновляем веса a и b в ходе одной итерации спуска \n",
    "    optimizer.apply_gradients(zip(gradients, [a, b]))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train(celsius, fahrenheit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение\n",
    "epochs = 2000 # число эпох \n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    # Делаем щаг градиентного спуска \n",
    "    model_train(celsius, fahrenheit)\n",
    "    \n",
    "    # Каждую сотую итерацию следим за тем, что произошло\n",
    "    if i%100 == 0:\n",
    "        y_pred = linear_regression(celsius)\n",
    "        loss_val = mean_square(y_pred, fahrenheit)\n",
    "        print(\"step: %i, loss: %f, a: %f, b: %f\" % (i, loss_val, a.numpy(), b.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно сделать то же самое, но с визуализацией. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "epochs = 2000 # число эпох \n",
    "\n",
    "ass = [a.numpy()[0]] # стартовые точки \n",
    "bss = [b.numpy()[0]] \n",
    "\n",
    "for i in range(epochs):\n",
    "    # Делаем щаг градиентного спуска \n",
    "    model_train(celsius, fahrenheit)\n",
    "    \n",
    "    if i%25 == 0:\n",
    "        # Пополняем запас точек \n",
    "        ass.append(a.numpy()[0])\n",
    "        bss.append(b.numpy()[0])\n",
    "\n",
    "        clear_output(True) # чтобы всё на одной картинке рисовалось, а не на милионе\n",
    "        plt.plot(ass, bss, marker='.')\n",
    "        plt.scatter(32, 1.8, c='red')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 5:\n",
    "\n",
    "Реализуйте для функции \n",
    "\n",
    "$$\n",
    "f(w) = \\prod_{i,j} \\ln(\\ln(w_{ij} + 7) \n",
    "$$\n",
    "\n",
    "процедуру градиентного спуска. Каким получилось минимальное значение? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваше решение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно попробовать поразвлекаться с разными методами оптимизации! \n",
    "\n",
    "* На вход модели при обучении мы сейчас подавали нумпаевские вектора. В будущем мы будем смотреть и на разные другие способы скармливать модели данные."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Нейронка на Tensorflow \n",
    "\n",
    "Пришло время страдать. Чтобы пострадать как следует, вы получаете задание --- построить свою первую нейросеть в tensorflow. __Спойлер:__ cильнее вего вы будете страдать от того, что размерности матриц не сходятся. Осторожнее с этим. \n",
    "\n",
    "Раньше все учили свою первую нейросеть на [наборе рукопистных цифр MNIST.](http://yann.lecun.com/exdb/mnist/) Теперь это не модно. Если где-то кто-то будет проводить у вас семинар с участием этого датасета, так и скажите ему: __это не модно!__  Хлопать дверью или нет --- на ваше усмотрение. \n",
    "\n",
    "На арене туториалов новый король: [Fashion MNIST.](https://www.tensorflow.org/tutorials/keras/classification) По ссылке лежит туториал с строительством нейросетки на этом наборе, но с участием Keras. Мы пока что обойдёмся без него."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нарисуем несколько рандомных картинок из тренировочной выборки. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = 8\n",
    "rows = 2\n",
    "fig = plt.figure(figsize=(2 * cols, 2.5 * rows))\n",
    "for i in range(cols):\n",
    "    for j in range(rows):\n",
    "        random_index = np.random.randint(0, len(y_train))\n",
    "        ax = fig.add_subplot(rows, cols, i * rows + j + 1)\n",
    "        ax.grid(False)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.imshow(X_train[random_index, :], cmap = 'gray')\n",
    "        ax.set_xlabel(class_names[y_train[random_index]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждая картинка это матрица из чисел. Если число большое - пиксель яркий. Если маленькое - тёмный. Яркость измеряется по шкале от $0$ до $255$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Картинка - матрица. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте отнормируем выборку. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train/255\n",
    "X_test = X_test/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем прогнозировать тип картинки по её пикселям. Давайте растянем её в вектор из $28 \\times 28$ фичей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 28**2)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28**2)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поставим для данных тип, чтобы ничего не поломалось."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас в задаче фигурирует $10$ классов. Последним слоем в сетке будет Softmax. Не забудьте сделать OHE, когда будете описывать функцию потерь. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 6\n",
    "\n",
    "Для начала обучим однослойною модель. Задайте её архитектуру и напишите функцию для прогнозирования. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10   # число классов\n",
    "num_features = 784 # число фичей (28*28 пикселей)\n",
    "\n",
    "# Задайте переменные W и b :) \n",
    "\n",
    "# прогнозы\n",
    "def logistic_regression(X):\n",
    "    # ваша модель\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_pred = logistic_regression(X_train[:3])\n",
    "p_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично! Теперь давайте реализуем функцию потерь. Использовать будем logloss. Ещё не забыли как его искать? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logloss(p_pred, y_true):\n",
    "    # Чтобы не возникло log(0) и бесконечностей\n",
    "    p_pred = tf.clip_by_value(p_pred, 1e-9, 1.)\n",
    "    # а теперь считаем\n",
    "    return -tf.reduce_mean(tf.reduce_sum(y_true * tf.math.log(p_pred), axis=1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы избежать проблем с бесконечностями, обычно logloss считают по-другому, с помощью функции tf.nn.softmax_cross_entropy_with_logits. Откуда берутся проблемы в виде бесконечностей в softmax либо посмотрите во второй лекции, либо в Николенко на страницах  133−135."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_pred = logistic_regression(X_train)\n",
    "logloss(p_pred, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На каждой итерации будем искать accuracy. Напишите функцию, чтобы его искать. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_true):\n",
    "    # Ваша реализация точности\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(p_pred, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задаём оптимизатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "optimizer = tf.optimizers.Adam(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задаём один шаг обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(X, Y):\n",
    "\n",
    "# Считаем градиенты\n",
    "    \n",
    "    # Обновляем веса a и b в ходе одной итерации спуска \n",
    "    optimizer.apply_gradients(zip(gradients, [W, b]))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Работает ли?\n",
    "model_train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Остался только этап для обучения модели. Напишем функцию, которая будет генерировать из наших данных батчи. Одна эпоха - один проход модели по всем батчам. Один батч - совокупность наблюдений.  В этом может помочь [специальный класс для создания батчей, Dataset.](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создали объект с данными для обучения \n",
    "train_data = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "\n",
    "# перемешали данные с окном в 5000 и сделали кучу батчей размера 512 \n",
    "train_data = train_data.shuffle(5000).batch(512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем небольшую функцию для визуализации процедуры обучения. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "def visualize(l1,l2, h1, h2):\n",
    "    plt.figure(figsize=(20,5)) \n",
    "    epo_range = range(1,len(h1)+1)\n",
    "    tick_range = range(1,len(h1)+1,2)\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title('Learning loss')\n",
    "    plt.plot(epo_range,l1, label='train set')\n",
    "    plt.plot(epo_range,l2, label='valid set')\n",
    "    plt.grid()\n",
    "    plt.xticks(tick_range)\n",
    "    plt.legend(title = 'Loss at:')\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title('Learning accuracy')\n",
    "    plt.plot(epo_range,h1, label='train set')\n",
    "    plt.plot(epo_range,h2, label='valid set')\n",
    "    plt.grid()\n",
    "    plt.xticks(tick_range)\n",
    "    plt.ylim(0, 1.)\n",
    "    plt.legend(title = 'Accuracy at:')\n",
    "    \n",
    "    display.clear_output(wait=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаем модель! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100           # число эпох \n",
    "training_steps = 1000  # сколько раз на эпохе теребить генератор батчей\n",
    "\n",
    "# Вектора для метрик\n",
    "loss_test, loss_train  = [ ], [ ] \n",
    "acc_test, acc_train = [ ], [ ]\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    # Проходимся по всем батчам\n",
    "    for batch_x, batch_y in train_data.take(training_steps):\n",
    "        \n",
    "        # Делаем щаг градиентного спуска \n",
    "        ### Ваш код для шага на текущем батче\n",
    "\n",
    "        ########################\n",
    "    \n",
    "    # в конце эпохи считаем метрики\n",
    "    p_train = logistic_regression(X_train)\n",
    "    p_test = logistic_regression(X_test)\n",
    "    \n",
    "    loss_train.append(logloss(p_train, y_train))\n",
    "    loss_test.append(logloss(p_test, y_test))\n",
    "    \n",
    "    acc_train.append(accuracy(p_train, y_train))\n",
    "    acc_test.append(accuracy(p_test, y_test))\n",
    "    \n",
    "    # визуализируем\n",
    "    visualize(loss_train, loss_test, acc_train, acc_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Делать такие визуализации для больших нейронок и большого количества эпох не лучшая идея, но так как мы только учимся и хотим красивых картинок, то мы можем себе это позволить. __Отдельно обратите внимание,__ что TensorFlow очень чуствительна к размерностям. Например, чтобы работала функция `matmul`, нужно подать ей на вход обязательно матрицы. Даже если это матрица размера $1 \\times 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 7\n",
    "\n",
    "Переделайте свою логистическую регрессию в двухслойную нейросетку. В качестве функции активации возьмите что угодно. Функцию, где будет реализована модель назовите `our_nn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаем модель!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100           # число эпох \n",
    "training_steps = 1000  # сколько раз на эпохе теребить генератор батчей\n",
    "\n",
    "# Вектора для метрик\n",
    "loss_test, loss_train  = [ ], [ ] \n",
    "acc_test, acc_train = [ ], [ ]\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    # Проходимся по всем батчам\n",
    "    for batch_x, batch_y in train_data.take(training_steps):\n",
    "        \n",
    "        # Делаем щаг градиентного спуска \n",
    "        model_train(batch_x, batch_y)\n",
    "    \n",
    "    p_train = our_nn(X_train)\n",
    "    p_test = our_nn(X_test)\n",
    "    \n",
    "    loss_train.append(logloss(p_train, y_train))\n",
    "    loss_test.append(logloss(p_test, y_test))\n",
    "    \n",
    "    acc_train.append(accuracy(p_train, y_train))\n",
    "    acc_test.append(accuracy(p_test, y_test))\n",
    "    \n",
    "    # визуализируем\n",
    "    visualize(loss_train, loss_test, acc_train, acc_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вместо итога.\n",
    "\n",
    "__Но... Но... Но... Намного проще собрать это в Keras! Зачем нам страдать?__\n",
    "\n",
    "Дело в том, что Keras лишь надстройка над tensorflow. По факту все слои, написанные для Keras - это замаскированный tensorflow. Более того, даже сетку, написанную на Keras можно обучать таким вот незатейливым циклом, как у нас. Иногда так намного проще проверять свои идеи. \n",
    "\n",
    "* Например, в прошлый раз мы с вами пытались мень скорость обучения сетки с помощью колбэков прямо в процессе обучения. Это было довольно неприятно и код был неочевидным. Если описывать процедуру обучения в виде цикла, работать становится проще.  В Tensorflow проще делать довольно много разных вещей, а также проще реализовывать и проверять какие-то свои идеи. \n",
    "* В том числе на Tensorflow можно писать новые слои для Keras и встраивать их в свои сетки. Этим мы тоже с вами позанимаемся."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N+1. Как Tensorflow работал раньше \n",
    "\n",
    "Больше такой код не надо писать никогда и нигде. Если видите его в туториалах в интернете, сразу закрывайте их. Эти туториалы устарели. Можете попробовать подуть на них (сдуть слой пыли), а дальше попробовать самостоятельно переписать старый код на новый. Это неплохое упражнение :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf  # подгружаем первую версию библиотеки \n",
    "tf.disable_v2_behavior()           # отключаем функционал второй \n",
    "                                   # теперь код, написанный на версии tf 1.x должен работать \n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Раньше сначала надо было задать структуру вычислений с помощью тензоров. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# что происходило в первой версии \n",
    "a = tf.constant([1, 2])\n",
    "b = tf.constant([3, 4])\n",
    "\n",
    "print(a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат на лету было увидеть нельзя. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a + b\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если хотелось результата, надо было запускать вычислительную сессию, которая прогоняла все данные через граф."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# открываем вычислительную сессию \n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# либо можно было открыть глобальную сессию \n",
    "sess = tf.InteractiveSession()\n",
    "c.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.003762897827421623"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "190/50493"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Процедура обучения модели выглядела раньше иначе. Кроме переменных в обучении участие принимал ещё один объект, `placeholder`. \n",
    "\n",
    "* __placeholder__ — место в графе, которое может принимать входные параметры в граф извне. В эти места мы будем вставлять наши данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "# плейсхолдеры для данных \n",
    "# \"Я обещаю вставить сюда x и y позже\"\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "# параметры модели \n",
    "a = tf.Variable(tf.zeros([1]), name='bias')\n",
    "b = tf.Variable(tf.zeros([1]), name='k')\n",
    "\n",
    "# модель \n",
    "y_hat = b*x + a\n",
    "\n",
    "# функция потерь и метод оптимизации\n",
    "loss = tf.sqrt(tf.reduce_sum((y - y_hat)**2))\n",
    "opt = tf.train.AdamOptimizer(learning_rate = 0.1)\n",
    "\n",
    "# из-за того, что не было динамических вычислений, \n",
    "# было проще с градиентами\n",
    "step = opt.minimize(loss)\n",
    "\n",
    "# Но зато было сложнее с процедурой обучения, надо было запускать ... сессии\n",
    "# открываем вычислительную сессию \n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # инициализировали все переменные \n",
    "    tf.global_variables_initializer().run() \n",
    "    \n",
    "    for i in range(epochs):\n",
    "        \n",
    "        # Дорогая сессия, вычисли мне результат функции потерь, пожалуйста! Вот тебе конкретные x и y! \n",
    "        cur_loss = sess.run(loss, feed_dict={x:celsius, y:fahrenheit})\n",
    "        print('Текущие потери:', cur_loss)\n",
    "        \n",
    "        # шаг оптимизации \n",
    "        sess.run(step, feed_dict={x:celsius, y:fahrenheit})\n",
    "        \n",
    "    print('\\nКоэффициенты:', a.eval()[0], b.eval()[0])\n",
    "    print('Прогнозы:', sess.run(y_hat, feed_dict={x:[-40,0,38]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Почиташки\n",
    "\n",
    "* [Эффективный tensorflow 2.0](https://www.tensorflow.org/guide/effective_tf2)\n",
    "* [Простые примеры кода на tensorflow 2.0](https://github.com/aymericdamien/TensorFlow-Examples/tree/master/tensorflow_v2)\n",
    "* [Очень-очень милая презентация про fashion mnist](https://github.com/fbchow/fashion-mnist-tensorflow/blob/master/ml-fashion-mnist-presentation.pdf)\n",
    "\n",
    "\n",
    "__Немного старья:__\n",
    "\n",
    "* [Введение в старую версию Tensorflow на Хабре](https://habrahabr.ru/company/ods/blog/324898/)\n",
    "* [Введение в старый Tensorflow от ШАД,](https://nbviewer.jupyter.org/github/yandexdataschool/Practical_DL/blob/fall18/week02_autodiff/seminar_tensorflow.ipynb) моя тетрадка частично основана на этой тетрадке, хоть сходство на первый взгляд и не заметно. \n",
    "* [Неплохая подборка разных тетрадок](https://github.com/Hvass-Labs/TensorFlow-Tutorials) на старом tensorflow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
